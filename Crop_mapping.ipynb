{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1UzrddcOVagZrMAjAgEFMeJWB3xwDfE8g",
      "authorship_tag": "ABX9TyNUqebAuHzXOKt8flS0+WIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ced-sys/.py/blob/main/Crop_mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28dgh-gnHW9O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "yP45ykTLJy8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "K3Agi_o_J_wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path='/content/drive/MyDrive/cropland-mapping'\n",
        "folders=['data', 'notebooks', 'submissions', 'src']"
      ],
      "metadata": {
        "id": "inxSFVjXKIrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Setting up project structure...\")\n",
        "for folder in folders:\n",
        "  folder_path=os.path.join(base_path, folder)\n",
        "  os.makedirs(folder_path, exist_ok=True)\n",
        "  print(f\"Created: {folder_path}\")"
      ],
      "metadata": {
        "id": "w9rLk6N2KogJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zindi_path='/content/drive/MyDrive/Zindi Hackathons'\n",
        "if os.path.exists(zindi_path):\n",
        "  print(f\"Found zindi hackathons folder at : {zindi_path}\")\n",
        "  print(\"Contents:\")\n",
        "  for item in os.listdir(zindi_path):\n",
        "    print(f\" -{item}\")\n",
        "else:\n",
        "  print(\"Zindi Hackathons folder not found. Please check the path\")"
      ],
      "metadata": {
        "id": "32QXWsT2K-Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  train_data=None\n",
        "  test_data=None\n",
        "\n",
        "  for root, dirs, files in os.walk(zindi_path):\n",
        "    for file in files:\n",
        "      if 'train' in file.lower() and (file.endswith('.csv') or file.endswith('.parquet')):\n",
        "        train_path=os.path.join(root, file)\n",
        "        print(f\"Found potential training data: {train_path}\")\n",
        "\n",
        "        if file.endswith('.csv'):\n",
        "          train_data=pd.read_csv(train_path)\n",
        "        elif file.endswith('.parquet'):\n",
        "          train_data=pd.read_parquet(train_path)\n",
        "        break\n",
        "\n",
        "      if 'test' in file.lower() and (file.endswith('.csv')or file.endswith('.parquet')):\n",
        "        test_path=os.path.join(root, file)\n",
        "        print(f\"Found potential test data: {test_path}\")\n",
        "\n",
        "        if file.endswith('.csv'):\n",
        "          test_data=pd.read_csv(test_path)\n",
        "        elif file.endswith('.parquet'):\n",
        "          test_data=pd.read_parquet(test_path)\n",
        "\n",
        "  if train_data is not None:\n",
        "    print(f\"Training data loaded: {train_data.shape}\")\n",
        "    print(f\"Columns:{list(train_data.columns)}\")\n",
        "    print(f\"First few rows:\")\n",
        "    print(train_data.head())\n",
        "\n",
        "    #Check for target column\n",
        "    target_columns=['target', 'Target', 'label', 'Label', 'class', 'Class']\n",
        "    target_col=None\n",
        "    for col in target_cols:\n",
        "      if col in train_data.columns:\n",
        "        target_col=col\n",
        "        break\n",
        "\n",
        "    if target_col:\n",
        "      print(f\"Target Column found: {target_col}\")\n",
        "      print(f\"Target distribution:\")\n",
        "      print(train_data[target_col].value_counts())\n",
        "    else:\n",
        "      print(\"Target column ouldnotbe immediately identified\")\n",
        "\n",
        "  if test_data is not None:\n",
        "    print(f\"Test data loaded: {test_data.shape}\")\n",
        "    print(f\"Columns: {list(test_data.columns)}\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Error loading data: {e}\")\n",
        "  print(\"Check the file paths\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5FQyLM71Ll9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLB8Uu8bFARN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dedc6dd6"
      },
      "source": [
        "# Task\n",
        "Extract the contents of the zipped file \"Train.zip\", which contains a folder of satellite training data, and prepare it for integration with the existing test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c069703f"
      },
      "source": [
        "## Explore extracted data\n",
        "\n",
        "### Subtask:\n",
        "Examine the contents of the extracted folder to understand the file types and structure of the satellite data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7644adb0"
      },
      "source": [
        "**Reasoning**:\n",
        "List the contents of the directory where \"Train.zip\" was extracted to understand the file types and structure of the satellite data. Assuming \"Train.zip\" was extracted to a 'Train' folder within the `zindi_path`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d3b86d5"
      },
      "source": [
        "train_extracted_path = os.path.join(zindi_path, 'Train')\n",
        "\n",
        "if os.path.exists(train_extracted_path):\n",
        "    print(f\"Contents of {train_extracted_path}:\")\n",
        "    for item in os.listdir(train_extracted_path):\n",
        "        item_path = os.path.join(train_extracted_path, item)\n",
        "        print(f\"- {item} ({'Directory' if os.path.isdir(item_path) else 'File'})\")\n",
        "\n",
        "        # Examine contents of a sub-directory if it's a folder (sample the first one found)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"  Contents of {item}:\")\n",
        "            sub_items = os.listdir(item_path)\n",
        "            for sub_item in sub_items[:5]: # List only first 5 items\n",
        "                 print(f\"  - {sub_item}\")\n",
        "            if len(sub_items) > 5:\n",
        "                print(\"  ...\")\n",
        "else:\n",
        "    print(f\"Directory not found: {train_extracted_path}. Please ensure 'Train.zip' was extracted.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c95715ff"
      },
      "source": [
        "## Identify data loading method\n",
        "\n",
        "### Subtask:\n",
        "Based on the file types, determine the appropriate method to load the satellite data (e.g., specific libraries for geospatial data).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4df8995c"
      },
      "source": [
        "## Load and preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Load the satellite data and perform any necessary preprocessing steps (e.g., handling different bands, time series data).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba0c6ba"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the geopandas library, construct the full file path to the shapefiles, and load them into GeoDataFrames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afcb6b1a"
      },
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "# Construct paths to shapefiles\n",
        "fergana_shp_path = os.path.join(zindi_path, 'Train', 'Fergana_training_samples.shp')\n",
        "orenburg_shp_path = os.path.join(zindi_path, 'Train', 'Orenburg_training_samples.shp')\n",
        "\n",
        "# Load shapefiles into GeoDataFrames\n",
        "try:\n",
        "    fergana_train_gdf = gpd.read_file(fergana_shp_path)\n",
        "    print(\"Fergana training data loaded:\")\n",
        "    display(fergana_train_gdf.head())\n",
        "    print(\"Fergana columns:\", fergana_train_gdf.columns.tolist())\n",
        "    print(\"\\nFergana target distribution:\")\n",
        "    display(fergana_train_gdf['crop_type'].value_counts())\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Fergana shapefile: {e}\")\n",
        "    fergana_train_gdf = None\n",
        "\n",
        "try:\n",
        "    orenburg_train_gdf = gpd.read_file(orenburg_shp_path)\n",
        "    print(\"\\nOrenburg training data loaded:\")\n",
        "    display(orenburg_train_gdf.head())\n",
        "    print(\"Orenburg columns:\", orenburg_train_gdf.columns.tolist())\n",
        "    print(\"\\nOrenburg target distribution:\")\n",
        "    display(orenburg_train_gdf['crop_type'].value_counts())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Orenburg shapefile: {e}\")\n",
        "    orenburg_train_gdf = None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37b9dc18"
      },
      "source": [
        "**Reasoning**:\n",
        "The shapefiles were loaded successfully, but the target column is named 'Cropland' instead of 'crop_type'. I need to inspect the dataframes to confirm the target column and its values before proceeding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97c21971"
      },
      "source": [
        "**Reasoning**:\n",
        "An error occurred because `fergana_train_gdf` is None. This is likely due to the error handling in the previous step that set the variable to None if loading failed. However, based on the previous output, the shapefiles *did* load successfully, just with a different column name than expected. I need to re-load the shapefiles without the error handling that sets the variable to None, and then proceed with inspecting the correct target column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nYDcq_NGSv4"
      },
      "source": [
        "# Load shapefiles into GeoDataFrames again, without the error handling that sets to None\n",
        "fergana_train_gdf = gpd.read_file(os.path.join(zindi_path, 'Train', 'Fergana_training_samples.shp'))\n",
        "orenburg_train_gdf = gpd.read_file(os.path.join(zindi_path, 'Train', 'Orenburg_training_samples.shp'))\n",
        "\n",
        "print(\"Fergana training data info:\")\n",
        "fergana_train_gdf.info()\n",
        "print(\"\\nOrenburg training data info:\")\n",
        "orenburg_train_gdf.info()\n",
        "\n",
        "print(\"\\nFergana 'Cropland' value counts:\")\n",
        "display(fergana_train_gdf['Cropland'].value_counts())\n",
        "print(\"\\nOrenburg 'Cropland' value counts:\")\n",
        "display(orenburg_train_gdf['Cropland'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5a76781"
      },
      "source": [
        "## Integrate with test data\n",
        "\n",
        "### Subtask:\n",
        "Integrate the loaded training data with the existing test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0852d226"
      },
      "source": [
        "**Reasoning**:\n",
        "Concatenate the two training GeoDataFrames and prepare them for integration with the test data by ensuring a location column exists. Then, inspect both the combined training data and the test data to determine a common integration strategy, which appears to be based on location.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac981b4f"
      },
      "source": [
        "# 1. Concatenate the two training GeoDataFrames\n",
        "# Add a 'location' column to each GeoDataFrame before concatenating\n",
        "fergana_train_gdf['location'] = 'Fergana'\n",
        "orenburg_train_gdf['location'] = 'Orenburg'\n",
        "\n",
        "combined_train_gdf = pd.concat([fergana_train_gdf, orenburg_train_gdf], ignore_index=True)\n",
        "\n",
        "print(\"Combined training data loaded:\")\n",
        "display(combined_train_gdf.head())\n",
        "print(\"\\nCombined training data columns:\", combined_train_gdf.columns.tolist())\n",
        "print(\"\\nCombined training data info:\")\n",
        "combined_train_gdf.info()\n",
        "\n",
        "# 2. and 3. Ensure a column for relating to test data and examine structures\n",
        "# The 'location' column is added to the training data.\n",
        "# The test_data already has a 'location' column.\n",
        "# Examine the test data structure again for comparison\n",
        "print(\"\\nTest data structure:\")\n",
        "display(test_data.head())\n",
        "print(\"\\nTest data columns:\", test_data.columns.tolist())\n",
        "print(\"\\nTest data info:\")\n",
        "test_data.info()\n",
        "\n",
        "# The 'location' column seems to be the common key for a potential integration\n",
        "# based on the provided data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d756a0"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the previous output, the combined training data and the test data both have a 'location' column. However, the 'ID' columns are different (integer in training, string in test). A direct merge by 'ID' or 'location' alone is not sufficient for precise integration as there is no common unique identifier across both datasets. The training data has point geometries and the test data has translated lat/lon. The most appropriate integration strategy is likely spatial, by joining the test points to the training geometries based on location. This requires converting the test data into a GeoDataFrame first.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "263decdf"
      },
      "source": [
        "from shapely.geometry import Point\n",
        "import geopandas as gpd\n",
        "\n",
        "# 5. Convert test_data to a GeoDataFrame\n",
        "# Create a 'geometry' column from 'translated_lon' and 'translated_lat'\n",
        "geometry = [Point(xy) for xy in zip(test_data['translated_lon'], test_data['translated_lat'])]\n",
        "test_gdf = gpd.GeoDataFrame(test_data, geometry=geometry, crs=combined_train_gdf.crs) # Use the same CRS as training data\n",
        "\n",
        "print(\"Test data converted to GeoDataFrame:\")\n",
        "display(test_gdf.head())\n",
        "print(\"\\nTest GeoDataFrame columns:\", test_gdf.columns.tolist())\n",
        "print(\"\\nTest GeoDataFrame info:\")\n",
        "test_gdf.info()\n",
        "\n",
        "# Perform a spatial join between the test points and training geometries based on location\n",
        "# This assumes the training geometries represent areas/points that the test points should be associated with.\n",
        "# We can join test points to training geometries that they intersect or are within.\n",
        "# A simple approach is to find which training geometry each test point falls within.\n",
        "# Since the training data contains geometries (likely points or polygons representing samples),\n",
        "# and the test data contains points, a spatial join (sjoin) is appropriate.\n",
        "\n",
        "# Perform spatial join based on location and geometry\n",
        "# First, ensure both GeoDataFrames have a common column for location to filter before joining\n",
        "integrated_data = gpd.sjoin(test_gdf, combined_train_gdf, how=\"left\", predicate=\"intersects\")\n",
        "\n",
        "print(\"\\nIntegrated data after spatial join:\")\n",
        "display(integrated_data.head())\n",
        "print(\"\\nIntegrated data columns:\", integrated_data.columns.tolist())\n",
        "print(\"\\nIntegrated data info:\")\n",
        "integrated_data.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d4fd77f"
      },
      "source": [
        "## Integrate with test data retry 1\n",
        "\n",
        "### Subtask:\n",
        "Integrate the loaded training data with the existing test data. This is a retry attempt after the initial spatial join was unsuccessful in linking test points to training geometries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dce4ecef"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the spatial join based on intersection failed, I will focus on making the training data information available alongside the test data using the common 'location' column, as suggested in the instructions. This involves ensuring both the combined training data and the test data are loaded and accessible, potentially grouped by location for future analysis or modeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fd4ed84"
      },
      "source": [
        "# Re-examine the 'location' column in both datasets to confirm it can serve as a key.\n",
        "print(\"Value counts for 'location' in combined training data:\")\n",
        "display(combined_train_gdf['location'].value_counts())\n",
        "\n",
        "print(\"\\nValue counts for 'location' in test data:\")\n",
        "display(test_data['location'].value_counts())\n",
        "\n",
        "# Ensure both dataframes are readily available.\n",
        "# combined_train_gdf and test_data (or test_gdf) are already loaded and available from previous steps.\n",
        "# The 'location' column exists in both and can be used for grouping or merging if needed later.\n",
        "\n",
        "print(\"\\nCombined training data head:\")\n",
        "display(combined_train_gdf.head())\n",
        "\n",
        "print(\"\\nTest data head:\")\n",
        "display(test_data.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8159fd3c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The extracted training data is in Esri shapefile format, consisting of `.shp`, `.shx`, and `.dbf` files, structured by location (`Fergana` and `Orenburg`).\n",
        "*   The `geopandas` library was successfully used to load the shapefiles into GeoDataFrames.\n",
        "*   The training data includes an `ID`, a `Cropland` column (the target variable with values 0 and 1), and geometry information.\n",
        "*   The two training GeoDataFrames were concatenated and a `location` column was added, successfully combining the Fergana and Orenburg training data.\n",
        "*   The test data was converted to a GeoDataFrame (`test_gdf`) using the provided latitude and longitude coordinates, and the Coordinate Reference System (CRS) was matched to the training data.\n",
        "*   An initial spatial join attempt between the test points and training geometries using the 'intersects' predicate failed to link test points to training labels effectively, resulting in almost entirely null values for training data columns in the integrated dataset.\n",
        "*   Both the combined training data and the test data contain a `location` column with the same unique values (`Fergana` and `Orenburg`), which can serve as a key for location-based analysis or grouping.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Since a direct spatial join for transferring training labels to test points was unsuccessful, alternative methods for using the training data are needed, potentially involving training a model on the training data and then using it to predict labels for the test points based on their attributes or surrounding features.\n",
        "*   Leverage the common `location` column to perform location-specific analysis or modeling, or to ensure that any integration or analysis respects the geographical origin of the data points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0265a0a7"
      },
      "source": [
        "## Integrate with test data\n",
        "\n",
        "### Subtask:\n",
        "Integrate the loaded training data with the existing test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b57beaa1"
      },
      "source": [
        "**Reasoning**:\n",
        "Concatenate the two training GeoDataFrames and prepare them for integration with the test data by ensuring a location column exists. Then, inspect both the combined training data and the test data to determine a common integration strategy, which appears to be based on location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d153a075"
      },
      "source": [
        "# 1. Concatenate the two training GeoDataFrames\n",
        "# Add a 'location' column to each GeoDataFrame before concatenating\n",
        "fergana_train_gdf['location'] = 'Fergana'\n",
        "orenburg_train_gdf['location'] = 'Orenburg'\n",
        "\n",
        "combined_train_gdf = pd.concat([fergana_train_gdf, orenburg_train_gdf], ignore_index=True)\n",
        "\n",
        "print(\"Combined training data loaded:\")\n",
        "display(combined_train_gdf.head())\n",
        "print(\"\\nCombined training data columns:\", combined_train_gdf.columns.tolist())\n",
        "print(\"\\nCombined training data info:\")\n",
        "combined_train_gdf.info()\n",
        "\n",
        "# 2. and 3. Ensure a column for relating to test data and examine structures\n",
        "# The 'location' column is added to the training data.\n",
        "# The test_data already has a 'location' column.\n",
        "# Examine the test data structure again for comparison\n",
        "print(\"\\nTest data structure:\")\n",
        "display(test_data.head())\n",
        "print(\"\\nTest data columns:\", test_data.columns.tolist())\n",
        "print(\"\\nTest data info:\")\n",
        "test_data.info()\n",
        "\n",
        "# The 'location' column seems to be the common key for a potential integration\n",
        "# based on the provided data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2nfElyBydHSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}