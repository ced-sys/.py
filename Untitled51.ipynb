{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNihfKBJJ1yHBCR3WzUBfu9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ced-sys/.py/blob/main/Untitled51.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ss-oTo0df7c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('default')\n",
        "sns.set_palette('husl')"
      ],
      "metadata": {
        "id": "rhp9ev4OfdTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "base_path='/content/drive/MyDrive/cropland-mapping'\n",
        "folders=['data', 'notebooks', 'submissions', 'src']\n",
        "\n",
        "for folder in folders:\n",
        "  folder_path=os.path.join(base_path, folder)\n",
        "  os.makedirs(folder_path, exist_ok=True)\n",
        "  print(f\"Created/Verified: {folder_path}\")\n",
        "\n",
        "print(\"\\nProject structure ready\")"
      ],
      "metadata": {
        "id": "CKYHVQC7fj94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas fiona shapely pyproj --quiet"
      ],
      "metadata": {
        "id": "3ly1d6wfnkyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path='/content/drive/MyDrive/Zindi Hackathons'\n",
        "\n",
        "import geopandas as gpd\n",
        "import glob\n",
        "import zipfile\n",
        "\n",
        "#load training data from Train folder\n",
        "train_folder=f'{data_path}/Train'\n",
        "\n",
        "#Check if Train.zip needs to be extracted\n",
        "if os.path.exists(f'{data_path}/Train.zip'):\n",
        "  print(\"Extracting train.zip...\")\n",
        "  with zipfile.ZipFile(f'{data_path}/Train.zip', 'r')as zip_ref:\n",
        "    zip_ref.extractall(data_path)\n",
        "\n",
        "#Find all shapefiles in the Train folder\n",
        "shapefile_paths=glob.glob(f'{train_folder}/**/*.shp', recursive=True)\n",
        "print(f\"Foud {len(shapefile_paths)} shapefiles:\")\n",
        "for path in shapefile_paths:\n",
        "  print(f\"  -{path}\")\n",
        "\n",
        "#Load training shapefiles\n",
        "training_data_list=[]\n",
        "for shp_path in shapefile_paths:\n",
        "  try:\n",
        "    gdf=gpd.read_file(shp_path)\n",
        "    region_name=os.path.basename(os.path.dirname(shp_path))\n",
        "    gdf['Region']=region_name\n",
        "    training_data_list.append(gdf)\n",
        "    print(f\"Loaded {region_name}: {gdf.shape[0]} samples\")\n",
        "    print(f\"Columns: {list(gdf.columns)}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading {shp_path}: {e}\")\n",
        "\n",
        "#Combine all training data\n",
        "if training_data_list:\n",
        "  train_gdf=pd.concat(training_data_list, ignore_index=True)\n",
        "  print(f\"\\n Combined trainig data shape: {train_gdf.shape}\")\n",
        "  print(f\"Training data columns: {list(train_gdf.columns)}\")\n",
        "else:\n",
        "  print(f\"Training Shapefiles loaded successfully\")\n",
        "\n",
        "train_sentinel1=pd.read_csv(f'{data_path}/Sentinel1.csv')\n",
        "train_sentinel2=pd.read_csv(f'{data_path}/Sentinel2.csv')\n",
        "\n",
        "#load test data (for predictions)\n",
        "test_data=pd.read_csv(f'{data_path}/Test.csv')\n",
        "\n",
        "sample_submission=pd.read_csv(f'{data_path}/SampleSubmission.csv')\n",
        "\n",
        "print(f\"\\nData Overview:\")\n",
        "print(f\"Training polygons:{train_gdf.shape[0]} samples\")\n",
        "print(f\"Sentinel-1 time series: {train_sentinel1.shape}\")\n",
        "print(f\"Sentinel-2 time series: {train_sentinel2.shape}\")\n",
        "print(f\"Test data: {test_data.shape}\")\n",
        "print(f\"Sample submission: {sample_submission.shape}\")\n",
        "\n",
        "#Peek at the training data structure\n",
        "print(f\"\\nTraining Data Structure:\")\n",
        "print(train_gdf.head())\n",
        "print(f\"\\nTraining data info:\")\n",
        "print(train_gdf.info())\n",
        "\n",
        "#Look for target/label column in training data\n",
        "possible_target_cols=['ID', 'Cropland', 'geometry', 'Region']\n",
        "target_col=None\n",
        "for col in possible_target_cols:\n",
        "  if col in train_gdf.columns:\n",
        "    target_col=col\n",
        "    break\n",
        "\n",
        "if target_col:\n",
        "  print(f\"\\nFound target column: '{target_col}'\")\n",
        "  print(f\"Target distribution:\")\n",
        "  print(train_gdf[target_col].value_counts())\n",
        "else:\n",
        "  print(f\"\\nTarget column not found. Available columns: {list(train_gdf.columns)}\")\n",
        "  #lLet's examine the data to understand the structure\n",
        "  print(f\"\\nFirst few rows of training data:\")\n",
        "  print(train_gdf.head())\n",
        "\n",
        "print(f\"\\nSentinel Data Structure:\")\n",
        "print(f\"\\nSentinel-1 columns (first 10): {train_sentinel1.columns.tolist()[:10]}\")\n",
        "print(f\"Sentinel-2 columns (first 10): {train_sentinel2.columns.tolist()[:10]}\")\n",
        "print(f\"Test data columns (first 10): {test_data.columns.tolist()[:10]}\")\n",
        "\n",
        "print(f\"\\nSample submission format:\")\n",
        "print(sample_submission.head())"
      ],
      "metadata": {
        "id": "9HU5wzr0gC9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_id_cols=[]\n",
        "if 'train_gdf' in locals():\n",
        "  for col in train_gdf.columns:\n",
        "    if col in train_sentinel1.columns or col in train_sentinel2.columns:\n",
        "      common_id_cols.append(col)\n",
        "\n",
        "if common_id_cols:\n",
        "  print(f\"\\nFound common ID columns:{common_id_cols}\")\n",
        "else:\n",
        "  print(f\"\\nNo obvious ID matching columns found. We may need to use spatial relationships\")"
      ],
      "metadata": {
        "id": "eZcQNIzc4oxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ndvi_time_series(df):\n",
        "    \"\"\"Calculate NDVI time series from NIR and Red bands for each unique ID.\"\"\"\n",
        "    # Define potential NIR and Red band columns, considering different naming conventions\n",
        "    nir_cols = [col for col in df.columns if 'B8' in col.upper() or 'NIR' in col.upper()]\n",
        "    red_cols = [col for col in df.columns if 'B4' in col.upper() or 'RED' in col.upper()]\n",
        "\n",
        "    if not nir_cols or not red_cols:\n",
        "        print(\"Warning: Could not find NIR or Red band columns.\")\n",
        "        return None\n",
        "\n",
        "    # Ensure 'ID' and 'date' columns exist\n",
        "    if 'ID' not in df.columns or 'date' not in df.columns:\n",
        "        print(\"Error: 'ID' and 'date' columns are required.\")\n",
        "        return None\n",
        "\n",
        "    # Sort data by ID and date to ensure correct time series order\n",
        "    df = df.sort_values(by=['ID', 'date'])\n",
        "\n",
        "    # Group by ID and calculate NDVI for each time step within the group\n",
        "    ndvi_list = []\n",
        "    for id, group in df.groupby('ID'):\n",
        "        ndvi_values = []\n",
        "        # Assuming first column in nir_cols is the primary NIR band and same for red_cols\n",
        "        nir = group[nir_cols[0]]\n",
        "        red = group[red_cols[0]]\n",
        "        # Calculate NDVI, handle division by zero\n",
        "        ndvi = (nir - red) / (nir + red + 1e-8)\n",
        "        ndvi_list.append({'ID': id, 'NDVI_values': ndvi.tolist()})\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    # This will result in a DataFrame where each row is an ID and a list of NDVI values\n",
        "    ndvi_df = pd.DataFrame(ndvi_list)\n",
        "\n",
        "    # We need to pivot this to have time steps as columns.\n",
        "    # This requires padding lists to the same length if time series are of different lengths.\n",
        "    max_len = ndvi_df['NDVI_values'].apply(len).max()\n",
        "    ndvi_padded = ndvi_df['NDVI_values'].apply(lambda x: x + [np.nan] * (max_len - len(x)))\n",
        "    ndvi_time_series_df = pd.DataFrame(ndvi_padded.tolist())\n",
        "    ndvi_time_series_df.columns = [f'NDVI_t{i:02d}' for i in range(max_len)]\n",
        "    ndvi_time_series_df['ID'] = ndvi_df['ID'] # Add ID back\n",
        "\n",
        "    print(f\"NDVI calculated: {max_len} time steps per ID (variable length time series padded with NaN)\")\n",
        "    return ndvi_time_series_df"
      ],
      "metadata": {
        "id": "u6U9aUWw7Jhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_time_series_features(df, time_cols, prefix=\"\"):\n",
        "  features=pd.DataFrame({'ID': df['ID']})\n",
        "  values=df[time_cols].values\n",
        "\n",
        "  #Statistical features\n",
        "  features[f'{prefix}mean']=np.mean(values, axis=1)\n",
        "  features[f'{prefix}std']=np.std(values, axis=1)\n",
        "  features[f'{prefix}min']=np.min(values, axis=1)\n",
        "  features[f'{prefix}max']=np.max(values, axis=1)\n",
        "  features[f'{prefix}range']=features[f'{prefix}max']-features[f'{prefix}min']\n",
        "  features[f'{prefix}trend']=np.polyfit(range(len(time_cols)), values.T, 1)[0]\n",
        "\n",
        "  #Seaonal features\n",
        "  features[f'{prefix}peak_idx']=np.argmax(values, axis=1)\n",
        "  features[f'{prefix}valley_idx']=np.argmin(values, axis=1)\n",
        "\n",
        "  return features\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "jW7sPCJO_h6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use spectral bands from train_sentinel2 for both train and test feature creation\n",
        "main_data = train_sentinel2\n",
        "spectral_cols = [col for col in main_data.columns if col not in ['ID', 'cloud_pct', 'date', 'solar_azimuth', 'solar_zenith', 'translated_lat', 'translated_lon']]\n",
        "\n",
        "# Create spectral features for training data\n",
        "features_df = create_time_series_features(main_data, spectral_cols, prefix=\"spectral_\")\n",
        "\n",
        "# Create spectral features for test data (using the same spectral columns)\n",
        "# We need to ensure test_data has the same structure as main_data for feature creation\n",
        "# However, test_data only has ID, location, translated_lat, translated_lon\n",
        "# Since test_data does not have the spectral bands, we cannot directly create spectral features for it in the same way.\n",
        "# We need to rethink how to handle the test data features.\n",
        "\n",
        "# Given the structure of the test data, it seems we cannot create time series features from it directly using the spectral bands.\n",
        "# We will proceed by creating features from the training data and then figure out how to handle the test data later,\n",
        "# possibly by matching IDs and using the same feature columns created from the training data.\n",
        "\n",
        "# For now, let's focus on creating features for the training data.\n",
        "print(f\"Spectral features created for training data: {features_df.shape}\")\n",
        "\n",
        "# We will need to address feature creation for the test data in a subsequent step.\n",
        "# Placeholder for test features - we will need to decide how to handle this based on the test data structure.\n",
        "test_features_df = pd.DataFrame({'ID': test_data['ID']})\n",
        "print(f\"Placeholder created for test features: {test_features_df.shape}\")\n",
        "\n",
        "feature_type = \"Spectral-based (Train only for now)\"\n",
        "\n",
        "print(f\"\\n{feature_type} features created: {features_df.shape} (train), {test_features_df.shape} (test - placeholder)\")"
      ],
      "metadata": {
        "id": "lDBPeoiOEBG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9def70f"
      },
      "source": [
        "# Convert 'ID' column in train_gdf to object type to match features_df\n",
        "train_gdf['ID'] = train_gdf['ID'].astype(str)\n",
        "\n",
        "# Aggregate features_df by ID\n",
        "# We need to decide on the aggregation method (e.g., mean, median, min, max, etc.)\n",
        "# For now, let's use the mean as an example. You might want to explore other aggregation methods later.\n",
        "aggregated_features_df = features_df.groupby('ID').mean().reset_index()\n",
        "\n",
        "# Merge training features and labels using an inner merge starting from train_gdf\n",
        "# This ensures we only keep IDs that are present in both the training labels and the aggregated features\n",
        "train_data = pd.merge(train_gdf[['ID', 'Cropland']], aggregated_features_df, on='ID', how='inner')\n",
        "\n",
        "print(\"Merged training data shape:\", train_data.shape)\n",
        "print(\"\\nMerged training data columns:\", list(train_data.columns))\n",
        "print(\"\\nFirst 5 rows of merged training data:\")\n",
        "display(train_data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_col=None\n",
        "for col in ['Cropland']:\n",
        "  if col in train_data.columns: # Changed from main_data to train_data\n",
        "    target_col=col\n",
        "    break\n",
        "\n",
        "if target_col:\n",
        "  print(f\"\\nFound target: '{target_col}'\")\n",
        "\n",
        "  # Drop rows with NaN in the target column before preparing data\n",
        "  train_data_cleaned = train_data.dropna(subset=[target_col])\n",
        "  print(f\"Training data shape after dropping NaNs: {train_data_cleaned.shape}\")\n",
        "\n",
        "  #Prepare training data\n",
        "  #features_with_target=features_df.merge(main_data[['ID', target_col]], on='ID') # Removed this line\n",
        "  X=train_data_cleaned.drop(['ID', target_col], axis=1) # Changed to use train_data_cleaned\n",
        "  y=train_data_cleaned[target_col] # Changed to use train_data_cleaned\n",
        "\n",
        "  print(f\"Training shape: {X.shape}\")\n",
        "  print(f\"Class Distribution: {y.value_counts().to_dict()}\") # Corrected value_count() to value_counts()\n",
        "\n",
        "  #Train Random Forest with cross-validation\n",
        "  rf=RandomForestClassifier(\n",
        "      n_estimators=100,\n",
        "      max_depth=10,\n",
        "      min_samples_split=10,\n",
        "      min_samples_leaf=5,\n",
        "      random_state=42,\n",
        "      n_jobs=-1\n",
        "  )\n",
        "\n",
        "  cv_scores=cross_val_score( # Corrected cv_sores to cv_scores\n",
        "      rf, X, y,\n",
        "      cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "      scoring='accuracy'\n",
        "  )\n",
        "  print(f\"\\nCross-validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()* 2:.4f})\")\n",
        "\n",
        "  #train final model\n",
        "  rf.fit(X, y)\n",
        "\n",
        "  #Feature importance\n",
        "  feature_importance=pd.DataFrame({\n",
        "      'feature':X.columns,\n",
        "      'importance':rf.feature_importances_\n",
        "  }).sort_values('importance', ascending=False)\n",
        "\n",
        "  print(f\"\\nTop 5 important Features:\")\n",
        "  print(feature_importance.head())\n",
        "\n",
        "  #VISUALIZATION\n",
        "  fig, axes=plt.subplots(2, 2, figsize=(15, 10))\n",
        "  fig.suptitle('Cropland Mapping Analysis', fontsize=16)\n",
        "\n",
        "  #class distribution\n",
        "  y.value_counts().plot(kind='pie', ax=axes[0,0], autopct='1.1f%%')\n",
        "  axes[0, 0].set_title('CLass Distribution')\n",
        "\n",
        "  #Feature distribution by class\n",
        "  top_feature=feature_importance.iloc[0]['feature']\n",
        "  # Use train_data_cleaned for plotting feature distribution\n",
        "  train_data_cleaned[train_data_cleaned[target_col]==0][top_feature].hist(\n",
        "      bins=30, alpha=0.7, label='Non-Cropland', ax=axes[1, 0]\n",
        "  )\n",
        "  train_data_cleaned[train_data_cleaned[target_col]==1][top_feature].hist(\n",
        "      bins=30, alpha=0.7, label='Cropland', ax=axes[1, 0]\n",
        "  )\n",
        "  axes[1, 0].set_title(f'{top_feature} Distribution')\n",
        "  axes[1, 0].legend()\n",
        "\n",
        "  #CV scores\n",
        "  axes[1, 1].bar(range(1, 6), cv_scores)\n",
        "  axes[1, 1].axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean: {cv_scores.mean():.3f}')\n",
        "  axes[1, 1].set_title('Cross-Validation Scores')\n",
        "  axes[1, 1].set_xlabel('Fold')\n",
        "  axes[1, 1].legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  #Make predictions\n",
        "  # We still need to create test_features_df with the same columns as X\n",
        "  # For now, we will use the placeholder test_features_df, which only has 'ID'\n",
        "  # This will cause an error in prediction, which we will address next.\n",
        "  test_X=test_features_df.drop(['ID'], axis=1)\n",
        "  predictions=rf.predict(test_X)\n",
        "\n",
        "  #Create Submission\n",
        "  submission=pd.DataFrame({\n",
        "      'ID':test_features_df['ID'],\n",
        "      'Target':predictions\n",
        "  })\n",
        "\n",
        "  print(f\"\\nPredictions completed\")\n",
        "  print(f\"Cropland predictions: {sum(predictions)} ({sum(predictions)/len(predictions)*100:.1f}%)\")\n",
        "\n",
        "  #Save submission\n",
        "  submission_path=f'{base_path}/submissions/baseline_submission_day1.csv'\n",
        "  submission.to_csv(submission_path, index=False)\n",
        "  print(f\"Submission saved to: {submission_path}\")"
      ],
      "metadata": {
        "id": "RnmHNPQqIJv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7641071"
      },
      "source": [
        "# Save submission to Colab runtime\n",
        "runtime_submission_path = '/tmp/baseline_submission_day1.csv'\n",
        "submission.to_csv(runtime_submission_path, index=False)\n",
        "print(f\"Submission also saved to Colab runtime: {runtime_submission_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}