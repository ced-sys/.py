{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOmjFT4GAZUThgkXD9L5Y1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ced-sys/.py/blob/main/Untitled52.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeuEUdNiX4rP"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas folium scikit-learn matplotlib seaborn shapely fiona"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "niaDnZ6VkpT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lCKjoG37lUnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sentinel_data(s1_path, s2_path):\n",
        "  print(\"Loading Sentinel-1 data...\")\n",
        "  s1_data=pd.read_csv(s1_path)\n",
        "  print(f\"Sentinel-1 shape: {s1_data.shape}\")\n",
        "  print(\"Sentinel-1 columns:\", list(s1_data.columns))\n",
        "\n",
        "  print(\"\\nLoading Sentinel-2 data...\")\n",
        "  s2_data=pd.read_csv(s2_path)\n",
        "  print(f\"Sentinel-2 shape: {s2_data.shape}\")\n",
        "  print(\"Sentinel-2 columns:\", list(s2_data.columns))\n",
        "\n",
        "  #Convert date columns\n",
        "  s1_data['date']=pd.to_datetime(s1_data['date'])\n",
        "  s2_data['date']=pd.to_datetime(s2_data['date'])\n",
        "\n",
        "  print(\"\\nData loaded successfully\")\n",
        "  return s1_data, s2_data"
      ],
      "metadata": {
        "id": "mwhjN_QYld3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_training_shapefiles(train_folder_path):\n",
        "  import os\n",
        "  import glob\n",
        "\n",
        "  print(f\"Loading shapefiles from: {train_folder_path}\")\n",
        "\n",
        "  #Find all shapefiles in the folder\n",
        "  shapefile_pattern=os.path.join(train_folder_path, \"*.shp\")\n",
        "  shapefile_paths=glob.glob(shapefile_pattern)\n",
        "\n",
        "  if not shapefile_paths:\n",
        "    print(\"No shapefiles found, Please check the folder path.\")\n",
        "    return {}\n",
        "\n",
        "  training_shapes={}\n",
        "\n",
        "  for shp_path in shapefile_paths:\n",
        "    region_name=os.path.basename(shp_path).replace('.shp', '')\n",
        "    print(f\"Loading {region_name}...\")\n",
        "\n",
        "    try:\n",
        "      gdf=gpd.read_file(shp_path)\n",
        "      training_shapes[region_name]=gdf\n",
        "      print(f\"   -Shape: {gdf.shape}\")\n",
        "      print(f\"   -Columns: {list(gdf.columns)}\")\n",
        "      print(f\"   -CRS: {gdf.crs}\")\n",
        "\n",
        "\n",
        "      #Print unique classes if available\n",
        "      class_columns=[col for col in gdf.columns\n",
        "                     if 'class' in col.lower() or 'land' in col.lower() or 'crop' in col.lower()]\n",
        "      for col in class_columns:\n",
        "        print(f\"  -Unique values in {col}: {gdf[col].unique()}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading {shp_path}: {e}\")\n",
        "\n",
        "\n",
        "  print(f\"\\nLoaded {len(training_shapes)} shapefiles\")\n",
        "  return training_shapes"
      ],
      "metadata": {
        "id": "qt5qGK6knA7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_satellite_data(s1_data, s2_data):\n",
        "  print(\"\\n--- SENTINEL-1 DATA ---\")\n",
        "  print(f\"Date range: {s1_data['date'].min()} to {s1_data['date'].max()}\")\n",
        "  print(f\"Unique IDs: {s1_data['ID'].nunique()}\")\n",
        "  print(f\"Unique orbits: {s1_data['orbit'].nunique() if 'orbit' in s1_data.columns else 'N/A'}\")\n",
        "\n",
        "\n",
        "  #Check for missing values\n",
        "  print(\"\\nMissing values:\")\n",
        "  print(s1_data.isnull().sum())\n",
        "\n",
        "  #Stats for VH and VV\n",
        "  print(\"\\nVH/VV Statistics:\")\n",
        "  print(s1_data[['VH', 'VV']].describe())\n",
        "\n",
        "  #Sentinel-2 exploration\n",
        "  print(\"\\n--- SENTINEL-2 DATA ---\")\n",
        "  print(f\"Date range: {s2_data['date'].min()} to {s2_data['date'].max()}\")\n",
        "  print(f\"Unique IDs: {s2_data['ID'].nunique()}\")\n",
        "\n",
        "  #Check for missing values\n",
        "  print(\"\\nMissing values:\")\n",
        "  print(s2_data.isnull().sum())\n",
        "\n",
        "  #Spectral bands analysis\n",
        "  spectral_bands=['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n",
        "  available_bands=[band for band in spectral_bands if band in s2_data.columns]\n",
        "  print(f\"\\nAvailabel spectral bands: {available_bands}\")\n",
        "\n",
        "  if available_bands:\n",
        "    print(\"\\nSpectral band statistics:\")\n",
        "    print(s2_data[available_bands].describe())\n",
        "\n",
        "  #Cloud coverage analysis\n",
        "  if 'cloud_pct' in s2_data.columns:\n",
        "    print(f\"\\nCloud coverage statistics:\")\n",
        "    print(s2_data['cloud_pct'].describe())\n",
        "    print(f\"Clear observations (<20% cloud): {(s2_data['cloud_pct']<20).mean()*100:.1f}%\")\n",
        "\n",
        "  #Solar angle analysis\n",
        "  solar_cols=['solar_azimuth', 'solar_zenith']\n",
        "  available_solar=[col for col in solar_cols if col in s2_data.columns]\n",
        "  if available_solar:\n",
        "    print(f\"\\nSolar angle statistics:\")\n",
        "    print(s2_data[available_solar].describe())\n"
      ],
      "metadata": {
        "id": "6nQQMhIyrNwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_temporal_features(data, id_col='ID', date_col='date', value_cols=None):\n",
        "  if value_cols is None:\n",
        "    #Auto-detect value columns\n",
        "    exclude_cols=[id_col, date_col, 'translated_lat', 'translated_lon',\n",
        "                  'orbit', 'polarization', 'rel_orbit', 'cloud_pct',\n",
        "                  'solar_azimuth', 'solar_zenith']\n",
        "    value_cols=[col for col in data.columns\n",
        "                if col not in exclude_cols and\n",
        "                data[col].dtype in ['float64', 'int64']]\n",
        "\n",
        "  print(f\"Creating temporal features for columns: {value_cols}\")\n",
        "\n",
        "  features_list=[]\n",
        "\n",
        "  for point_id in data[id_col].unique():\n",
        "    point_data=data[data[id_col]==point_id].sort_values(date_col)\n",
        "\n",
        "    if len(point_data)<2:\n",
        "      continue\n",
        "\n",
        "\n",
        "    feature_dict={id_col: point_id}\n",
        "\n",
        "\n",
        "    #Add location\n",
        "    feature_dict['lat']=point_data['translated_lat'].iloc[0]\n",
        "    feature_dict['lon']=point_data['translated_lon'].iloc[0]\n",
        "\n",
        "    #Temporal metadata\n",
        "    feature_dict['n_observations']=len(point_data)\n",
        "    feature_dict['date_range_days']=(point_data[date_col].max()-\n",
        "                                     point_data[date_col].min()).days\n",
        "    feature_dict['first_obs']=point_data[date_col].min().dayofyear\n",
        "    feature_dict['last_obs']=point_data[date_col].max().dayofyear\n",
        "\n",
        "    #Statistcial features for eah value column\n",
        "    for col in value_cols:\n",
        "      if col in point_data.columns:\n",
        "        values=point_data[col].dropna()\n",
        "        if len(values)>0:\n",
        "          feature_dict[f'{col}_mean']=values.mean()\n",
        "          feature_dict[f'{col}_std']=values.std()\n",
        "          feature_dict[f'{col}_min']=values.min()\n",
        "          feature_dict[f'{col}_max']=values.max()\n",
        "          feature_dict[f'{col}_median']=values.median()\n",
        "          feature_dict[f'{col}_range']=values.max()-values.min()\n",
        "\n",
        "          #Percentiles\n",
        "          feature_dict[f'{col}_p25']=values.quantile(0.25)\n",
        "          feature_dict[f'{col}_p75']=values.quantile(0.75)\n",
        "\n",
        "          #Temporal trend(slope of linear fit)\n",
        "          if len(values)>1:\n",
        "            x=np.arange(len(values))\n",
        "            slope=np.polyfit(x, values, 1)[0]\n",
        "            feature_dict[f'{col}_slope']=slope\n",
        "\n",
        "          #Coefficient of variation\n",
        "          if values.mean()!=0:\n",
        "            feature_dict[f'{col}_cv']=values.std()/ values.mean()\n",
        "\n",
        "    #add cloud coverage statistics for s2 data\n",
        "    if 'cloud_pct' in point_data.columns:\n",
        "      cloud_values=point_data['cloud_pct'].dropna()\n",
        "      if len(cloud_values)>0:\n",
        "        feature_dict['cloud_pct_mean']=cloud_values.mean()\n",
        "        feature_dict['cloud_pct_min']=cloud_values.min()\n",
        "        feature_dict['cloud_pct_max']=cloud_values.max()\n",
        "        feature_dict['cloud_obs_ratio']=(cloud_values<20).mean()\n",
        "\n",
        "    #Add cloud coverage statistics for s2 data\n",
        "    for angle_col in ['solar_azimuth', 'solar_zenit']:\n",
        "      if angle_col in point_data.columns:\n",
        "        angle_values=point_data[angle_col].dropna()\n",
        "        if len(angle_values)>0:\n",
        "          feature_dict[f'{angle_col}_mean']=angle_values.mean()\n",
        "          feature_dict[f'{angle_col}_std']=angle_values.std()\n",
        "\n",
        "    features_list.append(feature_dict)\n",
        "\n",
        "  return pd.DataFrame(features_list)\n"
      ],
      "metadata": {
        "id": "iczD_1_ltbOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_vegetation_indices(s2_data):\n",
        "  print(\"Calculating vegetation indices...\")\n",
        "\n",
        "  s2_enhanced=s2_data.copy()\n",
        "\n",
        "  #NDVI (Normalized Difference Vegetation Indices)\n",
        "  if 'B4' in s2_data.columns and 'B8' in s2_data.columns:\n",
        "    s2_enhanced['NDVI']=(s2_data['B8']-s2_data['B4'])/ (s2_data['B8']+s2_data['B4']+1e-8)\n",
        "\n",
        "    #EVI (Enhanced Vegetation Index)\n",
        "    if all(band in s2_data.columns for band in ['B2', 'B4', 'B8']):\n",
        "      s2_enhanced['EVI']=2.5 ((s2_data['B8']-s2_data['B4'])/\n",
        "                              (s2_data['B8']+6* s2_data['B4']-7.5*s2_data['B2']+1))\n",
        "\n",
        "    #NDWI (Normalized Difference Water Index)\n",
        "    if 'B3' in s2_data.columns and 'B8' is s2_data.columns:\n",
        "      s2_enhanced['NDWI']=(s2_data['B3']-s2_data['B8'])/ (s2_data['B3']+s2_data['B8']+1e-8)\n",
        "\n",
        "    #SAVI (Soil Adjusted Vegetation Index)\n",
        "    if 'B4' in s2_data.columns and 'B8' in s2_data.columns:\n",
        "      L=0.5\n",
        "      s2_enhanced['SAVI']=((s2_data['B8']-s2_data['B4'])/\n",
        "                           (s2_data['B8']+s2_data['B4']+L))* (1+L)\n",
        "\n",
        "    #NDRE (Normalized Difference Red Edge)\n",
        "    if 'B5' in s2_data.columns and 'B8' in s2_data.columns:\n",
        "      s2_enhanced['NDRE']=(s2_data['B8']-s2_data['B5'])/ (s2_data['B8']+s2_data['B5']+1e-8)\n",
        "\n",
        "    #MCARI (Modified Chlorophyll Absorption Index)\n",
        "    if all(band in s2_data.columns for band in ['B3', 'B4', 'B5']):\n",
        "      s2_enhanced['MCARI']=((s2_data['B5']-s2_data['B4'])-0.2*\n",
        "                            (s2_data['B5']-s2_data['B3']))* (s2_data['B5']/ s2_data['B4'])\n",
        "\n",
        "    #NBR (Normalized Burn ratio) - useful for detecting bare soil\n",
        "    if 'B8' in s2_data.columns and 'B4' in s2_data.columns:\n",
        "      s2_enhanced['NBR']=(s2_data['B8']-s2_data['B12'])/(s2_data['B8']+s2_data['B12']+1e-8)\n",
        "\n",
        "\n",
        "    #Simple band ratios\n",
        "    if 'B8' in s2_data.columns and 'B4' in s2_data.columns:\n",
        "      s2_enhanced['NIR_Red_ratio']=s2_data['B8']/ (s2_data['B4']+1e-8)\n",
        "\n",
        "    if 'B11' in s2_data.columns and 'B12' in s2_data.columns:\n",
        "      s2_enhanced['SWIR_SWIR2_ratio']=s2_data['B11']/ (s2_data['B12']+1e-8)\n",
        "\n",
        "\n",
        "    print(f\"Added {len(s2_enhanced.columns)- len(s2_data.columns)} vegetation indices\")\n",
        "    return s2_enhanced\n",
        "\n"
      ],
      "metadata": {
        "id": "74dSM-gQz_VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_satellite_data(s1_data, s2_data):\n",
        "  print(\"Processing Sentinel-1 features...\")\n",
        "  s1_features=create_temporal_features(s1_data, value_cols=['VH', 'VV'])\n",
        "\n",
        "  print('Processing Sentinel-2 features...')\n",
        "  #Get s2 Spectral columns\n",
        "  s2_spectral_bands=['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n",
        "  #include cloud and solar angle information\n",
        "  s2_value_cols=s2_spectral_bands+['cloud_pct', 'solar_azimuth', 'solar_zenith']\n",
        "  #Only use columns that exist in the data\n",
        "  available_s2_cols=[col for col in s2_value_cols if col in s2_data.columns]\n",
        "  print(f\"Available S2 columns: {available_s2_cols}\")\n",
        "\n",
        "  s2_features=create_temporal_features(s2_data, value_cols=available_s2_cols)\n",
        "\n",
        "  #Merge S1 and S2 features\n",
        "  print(\"Merging S1 and S2 features...\")\n",
        "  all_features=pd.merge(s1_features, s2_features, on='ID', how='outer', suffixes=('_s1', '_s2'))\n",
        "\n",
        "  #Handle coordinate duplicates\n",
        "  if 'lat_s1' in all_features.columns and 'lat_s2' in all_features.columns:\n",
        "    all_features['lat']=all_features['lat_s1'].fillna(all_features['lat_s2'])\n",
        "    all_features['lon']=all_features['lon_s1'].fillna(all_features['lon_s2'])\n",
        "    all_features=all_features.drop(['lat_s1', 'lat_s2', 'lon_s1', 'lon_s2'], axis=1)\n",
        "\n",
        "  print(f\"Final feature matrix shape: {all_features.shape}\")\n",
        "  return all_features"
      ],
      "metadata": {
        "id": "OO9RI7MdylpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spatial_join_training_data(s1_data, training_shapes, buffer_distance=0.001):\n",
        "  from shapely.geometry import Point\n",
        "\n",
        "  print(\"Performing spatial join with training data...\")\n",
        "\n",
        "  #Combine all training data\n",
        "  all_training=[]\n",
        "  for region, gdf in training_shapes.items():\n",
        "    gdf_copy=gdf.copy()\n",
        "    gdf_copy['region']=region\n",
        "    all_training.append(gdf_copy)\n",
        "\n",
        "  if not all_training:\n",
        "    print(\"No training data available\")\n",
        "    return None\n",
        "\n",
        "  combined_training=gpd.GeoDataFrame(pd.concat(all_training, ignore_index=True))\n",
        "\n",
        "  #Create a point geometries from satellite data coordinates\n",
        "  unique_points=s1_data[['ID', 'translated_lat', 'translated_lon']].drop_duplicates()\n",
        "\n",
        "  geometry=[Point(lon, lat) for lon, lat in zip(unique_points['translated_lon'],\n",
        "                                                unique_points['translated_lat'])]\n",
        "\n",
        "  points_gdf=gpd.GeoDataFrame(unique_points, geometry=geometry)\n",
        "\n",
        "  #Set CRS (assuming WGS84- adjust if needed)\n",
        "  points_gdf.crs='EPSG:4326'\n",
        "  if combined_training.crs is  None:\n",
        "    combined_training.crs='EPSG:4326'\n",
        "\n",
        "  #Perform spatial join\n",
        "  print(\"Executing spatial join...\")\n",
        "  joined_data=gpd.sjoin(points_gdf, combined_training, how='inner', predicate='within')\n",
        "\n",
        "  if len(joined_data)==0:\n",
        "    print(\"No Spatial matches found! Trying with buffer...\")\n",
        "    #Buffer the training polygons\n",
        "    combined_training['geometry']=combined_training.geometry.buffer(buffer_distance)\n",
        "    joined_data=gpd.sjoin(points_gdf, combined_training, how='inner', predicate='within')\n",
        "\n",
        "  print(f\"Spatial join completed: {len(joined_data)} matches found\")\n",
        "  return joined_data"
      ],
      "metadata": {
        "id": "e-583FiQ7aAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_dataset(processed_features, spatial_joined_data):\n",
        "  #Find the class column in spatial joined data\n",
        "  potential_class_cols=[col for col in spatial_joined_data.columns\n",
        "                        if any (keyword in col.lower() for keyword in\n",
        "                                ['class', 'land', 'crop', 'type', 'cover'])]\n",
        "\n",
        "  if not potential_class_cols:\n",
        "    print(\"No class column found in training data\")\n",
        "    return None, None\n",
        "\n",
        "  class_col=potential_class_cols[0]\n",
        "  print(f\"Using '{class_col}' as the class column\")\n",
        "\n",
        "\n",
        "  #Merge features with labels\n",
        "  # Use 'ID_left' from spatial_joined_data as it contains the original satellite data ID\n",
        "  training_data=pd.merge(\n",
        "      processed_features,\n",
        "      spatial_joined_data[['ID_left', class_col, 'region']],\n",
        "      left_on='ID', # Merge processed_features on 'ID'\n",
        "      right_on='ID_left', # Merge spatial_joined_data on 'ID_left'\n",
        "      how='inner'\n",
        "  )\n",
        "\n",
        "  # Drop the redundant 'ID_left' column after merging\n",
        "  training_data = training_data.drop('ID_left', axis=1)\n",
        "\n",
        "\n",
        "  print(f\"Training dataset shape; {training_data.shape}\")\n",
        "  print(f\"Class Distribution:\")\n",
        "  print(training_data[class_col].value_counts())\n",
        "\n",
        "  return training_data, class_col"
      ],
      "metadata": {
        "id": "1zPGdQB5_VaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cropland_model(training_data, class_col):\n",
        "  print(\"Training the classification model...\")\n",
        "\n",
        "  #Prepare features and labels\n",
        "  feature_cols=[col for col in training_data.columns\n",
        "                if col not in ['ID', class_col, 'region']]\n",
        "  X=training_data[feature_cols]\n",
        "  y=training_data[class_col]\n",
        "\n",
        "  #Handle missing values\n",
        "  X=X.fillna(X.mean())\n",
        "\n",
        "  #Initialize components\n",
        "  scaler=StandardScaler()\n",
        "  label_encoder=LabelEncoder()\n",
        "\n",
        "  #Encode labels\n",
        "  y_encoded=label_encoder.fit_transform(y)\n",
        "\n",
        "  #Scale features\n",
        "  X_scaled=scaler.fit_transform(X)\n",
        "\n",
        "  #Split data\n",
        "  X_train, X_test, y_train, y_test=train_test_split(\n",
        "      X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "  )\n",
        "\n",
        "  #Train Random Forest Model\n",
        "  model=RandomForestClassifier(\n",
        "      n_estimators=200,\n",
        "      max_depth=15,\n",
        "      min_samples_split=5,\n",
        "      min_samples_leaf=2,\n",
        "      random_state=42,\n",
        "      n_jobs=-1\n",
        "  )\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  #Evaluate model\n",
        "  y_pred=model.predict(X_test)\n",
        "  accuracy=accuracy_score(y_test, y_pred)\n",
        "\n",
        "  print(f\"Model Accuracy: {accuracy:.3f}\")\n",
        "  print(\"\\nClassification Report:\")\n",
        "  # Ensure target_names is a list of strings\n",
        "  target_names_list = label_encoder.classes_.tolist()\n",
        "  print(classification_report(y_test, y_pred,\n",
        "                             target_names=target_names_list))\n",
        "\n",
        "  #Feature Importance\n",
        "  feature_importance=pd.DataFrame({\n",
        "      'feature':feature_cols,\n",
        "      'importance':model.feature_importances_\n",
        "  }).sort_values('importance', ascending=False)\n",
        "\n",
        "  #Cross-validation\n",
        "  cv_scores=cross_val_score(model, X_scaled, y_encoded, cv=5)\n",
        "  print(f\"\\nCross-validation scores: {cv_scores}\")\n",
        "  print(f\"Mean Cv score: {cv_scores.mean():3f}(+/-{cv_scores.std()*2:.3f})\")\n",
        "\n",
        "  #Return all components\n",
        "  return{\n",
        "      'model':model,\n",
        "      'scaler': scaler,\n",
        "      'label_encoder': label_encoder,\n",
        "      'feature_cols':feature_cols,\n",
        "      'X_test': X_test,\n",
        "      'y_test': y_test,\n",
        "      'y_pred': y_pred,\n",
        "      'feature_importance': feature_importance,\n",
        "      'accuracy': accuracy,\n",
        "      'cv_scores':cv_scores\n",
        "  }"
      ],
      "metadata": {
        "id": "aPzp3xtSAkY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_cropland(model_dict, new_features):\n",
        "  model=model_dict['model']\n",
        "  scaler=model_dict['scaler']\n",
        "  label_encoder=model_dict['label_encoder']\n",
        "  feature_cols=model_dict['feature_cols']\n",
        "\n",
        "  #Prepare features\n",
        "  X_new=new_features[feature_cols].fillna(new_features[feature_cols].mean())\n",
        "\n",
        "  #Scale features\n",
        "  X_new_scaled=scaler.transform(X_new)\n",
        "\n",
        "  #Make predictions\n",
        "  predictions=model.predict(X_new_scaled)\n",
        "  probabilities=model.predict_proba(X_new_scaled)\n",
        "\n",
        "  #Decode predictions\n",
        "  predicted_labels=label_encoder.inverse_transform(predictions)\n",
        "\n",
        "  return predicted_labels, probabilities\n"
      ],
      "metadata": {
        "id": "hNkYoaAjERWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_results(model_dict):\n",
        "\n",
        "  y_test=model_dict['y_test']\n",
        "  y_pred=model_dict['y_pred']\n",
        "  feature_importance=model_dict['feature_importance']\n",
        "  label_encoder=model_dict['label_encoder']\n",
        "\n",
        "  fig, axes=plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "  #Confusion matrix\n",
        "  cm=confusion_matrix(y_test, y_pred)\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "              xticklabels=label_encoder.classes_,\n",
        "              yticklabels=label_encoder.classes_,\n",
        "              ax=axes[0, 0])\n",
        "  axes[0, 0].set_title('Confusion Matrix')\n",
        "  axes[0, 0].set_ylabel('True Label')\n",
        "  axes[0, 0].set_xlabel('Predicted Label')\n",
        "\n",
        "  #feature Importance\n",
        "  top_features=feature_importance.head(15)\n",
        "  axes[0, 1].barh(top_features['feature'], top_features['importance'])\n",
        "  axes[0, 1].set_title('Top 15 Feature Importance')\n",
        "  axes[0, 1].set_xlabel('Importance')\n",
        "\n",
        "  #Class Distribution\n",
        "  class_counts=pd.Series(y_test).value_counts()\n",
        "  class_names=[label_encoder.classe_[i] for i in class_counts.index]\n",
        "  axes[1,0].pie(class_counts.values, labels=class_names, autopct='%1.1f%%')\n",
        "  axes[1,0].set_title('Test Set Class Distribution')\n",
        "\n",
        "  #Accuracy by class\n",
        "  from sklearn.metrics import classification_report\n",
        "  report=classification_report(y_test, y_pred, output_dict=True)\n",
        "  classes=list(report.keys())[:-3]\n",
        "  f1_scores=[report[cls]['f1_score'] for cls in classes]\n",
        "  class_labels=[label_encoder.classes_[int(cls)] if cls.isdigit() else cls for cls in classes]\n",
        "\n",
        "  axes[1, 1].bar(class_labels, f1_scores)\n",
        "  axes[1, 1].set_title('F1-Score by Class')\n",
        "  axes[1, 1].set_ylabel('F1-Score')\n",
        "  axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "p6UGZcwOFNqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_satellite_data(s1_data, s2_data, processed_features):\n",
        "  fig, axes=plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "  #1.Observation count distribution\n",
        "  if 'n_observations_s1' in processed_features.columns:\n",
        "    processed_features['n_observation_s1'].hist(bins=30, ax=axes[0, 0])\n",
        "    axes[0, 0].set_title(\"S1 Obswrvation Count Distribution\")\n",
        "    axes[0, 0].set_xlabel('Number of Observations')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "\n",
        "  #2.VH vs VV Scatter\n",
        "  if 'VH_mean' in processed_features.columns and 'VV_mean' in processed_features.columns:\n",
        "    axes[0, 1].scatter(processed_features['VH_mean'],\n",
        "                       processed_features['VV_mean'], alpha=0.5)\n",
        "    axes[0, 1].set_title('VH vs VV Mean Values')\n",
        "    axes[0, 1].set_xlabel('VH Mean')\n",
        "    axes[0, 1].set_ylabel('VV Mean')\n",
        "\n",
        "\n",
        "  #3. Geographic distribution\n",
        "  if 'lat' in processed_features.columns and 'lon' in processed_features.columns:\n",
        "    axes[0, 2].scatter(processed_features['lon'],\n",
        "                       processed_features['lat'], alpa=0.5)\n",
        "    axes[0, 2].set_title('Geographic Distribution of Points')\n",
        "    axes[0, 2].set_xlabel('Longitude')\n",
        "    axes[0, 2].set_ylabel('Latitude')\n",
        "\n",
        "  #4. Seasonal Patterns\n",
        "  if 'first_obs_s1' in processed_features.columns:\n",
        "    processed_features['first_obs_s1'].hist(bins=30, ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('First Observation Day of the Yeat (S1)')\n",
        "    axes[1, 0].set_xlabel('Day Of Year')\n",
        "\n",
        "\n",
        "  #5. Date range distribution\n",
        "  if 'date_range_days_s1' in processed_features.columns:\n",
        "    processed_features['date_range_days_s1'].hist(bins=30, ax=axes[1, 1])\n",
        "    axes[1,1].set_title('Observation Period Length (S1)')\n",
        "    axes[1, 1].set_xlabel('Days')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "\n",
        "  #6. VH standard deviation\n",
        "  if 'VH_std' in processed_features.columns:\n",
        "    processed_features['VH_std'].hist(bins=30, ax=axes[1, 2])\n",
        "    axes[1, 2].set_title('VH Standard Deviation')\n",
        "    axes[1, 2].set_xlabel('VH Std')\n",
        "    axes[1, 2].set_ylabel('Frequency')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "Aklen12pIQ8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_satellite_data(s1_data, s2_data, processed_features):\n",
        "    \"\"\"\n",
        "    Create visualizations of the satellite data\n",
        "\n",
        "    Args:\n",
        "        s1_data (DataFrame): Sentinel-1 data\n",
        "        s2_data (DataFrame): Sentinel-2 data\n",
        "        processed_features (DataFrame): Processed features\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
        "\n",
        "    # 1. Observation count distribution\n",
        "    if 'n_observations_s1' in processed_features.columns:\n",
        "        processed_features['n_observations_s1'].hist(bins=30, ax=axes[0,0])\n",
        "        axes[0,0].set_title('S1 Observation Count Distribution')\n",
        "        axes[0,0].set_xlabel('Number of Observations')\n",
        "        axes[0,0].set_ylabel('Frequency')\n",
        "\n",
        "    # 2. VH vs VV scatter\n",
        "    if 'VH_mean' in processed_features.columns and 'VV_mean' in processed_features.columns:\n",
        "        axes[0,1].scatter(processed_features['VH_mean'],\n",
        "                        processed_features['VV_mean'], alpha=0.5)\n",
        "        axes[0,1].set_title('VH vs VV Mean Values')\n",
        "        axes[0,1].set_xlabel('VH Mean')\n",
        "        axes[0,1].set_ylabel('VV Mean')\n",
        "\n",
        "    # 3. Geographic distribution\n",
        "    if 'lat' in processed_features.columns and 'lon' in processed_features.columns:\n",
        "        axes[0,2].scatter(processed_features['lon'],\n",
        "                        processed_features['lat'], alpha=0.5)\n",
        "        axes[0,2].set_title('Geographic Distribution of Points')\n",
        "        axes[0,2].set_xlabel('Longitude')\n",
        "        axes[0,2].set_ylabel('Latitude')\n",
        "\n",
        "    # 4. NDVI distribution\n",
        "    if 'NDVI_mean' in processed_features.columns:\n",
        "        processed_features['NDVI_mean'].hist(bins=30, ax=axes[1,0])\n",
        "        axes[1,0].set_title('NDVI Mean Distribution')\n",
        "        axes[1,0].set_xlabel('NDVI Mean')\n",
        "        axes[1,0].set_ylabel('Frequency')\n",
        "\n",
        "    # 5. Cloud coverage distribution\n",
        "    if 'cloud_pct_mean' in processed_features.columns:\n",
        "        processed_features['cloud_pct_mean'].hist(bins=30, ax=axes[1,1])\n",
        "        axes[1,1].set_title('Cloud Coverage Distribution')\n",
        "        axes[1,1].set_xlabel('Mean Cloud Percentage')\n",
        "        axes[1,1].set_ylabel('Frequency')\n",
        "\n",
        "    # 6. NIR vs Red scatter (B8 vs B4)\n",
        "    if 'B8_mean' in processed_features.columns and 'B4_mean' in processed_features.columns:\n",
        "        axes[1,2].scatter(processed_features['B4_mean'],\n",
        "                        processed_features['B8_mean'], alpha=0.5)\n",
        "        axes[1,2].set_title('NIR (B8) vs Red (B4) Mean Values')\n",
        "        axes[1,2].set_xlabel('Red (B4) Mean')\n",
        "        axes[1,2].set_ylabel('NIR (B8) Mean')\n",
        "\n",
        "    # 7. Seasonal patterns (S2)\n",
        "    if 'first_obs_s2' in processed_features.columns:\n",
        "        processed_features['first_obs_s2'].hist(bins=30, ax=axes[2,0])\n",
        "        axes[2,0].set_title('First Observation Day of Year (S2)')\n",
        "        axes[2,0].set_xlabel('Day of Year')\n",
        "        axes[2,0].set_ylabel('Frequency')\n",
        "\n",
        "    # 8. EVI vs NDVI comparison\n",
        "    if 'EVI_mean' in processed_features.columns and 'NDVI_mean' in processed_features.columns:\n",
        "        axes[2,1].scatter(processed_features['NDVI_mean'],\n",
        "                        processed_features['EVI_mean'], alpha=0.5)\n",
        "        axes[2,1].set_title('EVI vs NDVI Mean Values')\n",
        "        axes[2,1].set_xlabel('NDVI Mean')\n",
        "        axes[2,1].set_ylabel('EVI Mean')\n",
        "\n",
        "    # 9. SWIR band correlation (B11 vs B12)\n",
        "    if 'B11_mean' in processed_features.columns and 'B12_mean' in processed_features.columns:\n",
        "        axes[2,2].scatter(processed_features['B11_mean'],\n",
        "                        processed_features['B12_mean'], alpha=0.5)\n",
        "        axes[2,2].set_title('SWIR1 (B11) vs SWIR2 (B12)')\n",
        "        axes[2,2].set_xlabel('B11 Mean')\n",
        "        axes[2,2].set_ylabel('B12 Mean')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Additional plot for cloud-free observation analysis\n",
        "    if 'clear_obs_ratio' in processed_features.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        processed_features['clear_obs_ratio'].hist(bins=30)\n",
        "        plt.title('Clear Observation Ratio Distribution')\n",
        "        plt.xlabel('Ratio of Clear Observations (<20% cloud)')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        #Correlation between cloud coverage and vegetation indices\n",
        "        if 'NDVI_mean' in processed_features.columns and 'cloud_pct_mean' in processed_features.columns:\n",
        "          plt.scatter(processed_features['cloud_pct_mean'],\n",
        "                      processed_features['NDVI_mean'], alpha=0.5)\n",
        "          plt.title('Cloud Coverage vs NDVI')\n",
        "          plt.xlabel('Mean Cloud Percentage')\n",
        "          plt.ylabel('NDVI Mean')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7w1FiJ_Q6xBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1_path='/content/drive/MyDrive/Zindi Hackathons/Sentinel1.csv'\n",
        "s2_path='/content/drive/MyDrive/Zindi Hackathons/Sentinel2.csv'\n",
        "train_folder='/content/drive/MyDrive/Zindi Hackathons/Train'"
      ],
      "metadata": {
        "id": "L1K2bz8_Lclq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"File paths set:\")\n",
        "print(f\"Sentinel-1: {s1_path}\")\n",
        "print(f\"Sentinel-2: {s2_path}\")\n",
        "print(f\"Training folder: {train_folder}\")\n"
      ],
      "metadata": {
        "id": "Slwy0enebRwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1_data, s2_data=load_sentinel_data(s1_path, s2_path)\n",
        "\n",
        "training_shapes=load_training_shapefiles(train_folder)"
      ],
      "metadata": {
        "id": "mhfEdx5Ubn_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic data exploration\n",
        "explore_satellite_data(s1_data, s2_data)\n",
        "\n",
        "print(\"\\nSentinel-1 sample:\")\n",
        "print(s1_data.head())\n",
        "\n",
        "print(\"\\nSentinel-2 sample:\")\n",
        "print(s2_data.head())\n",
        "\n",
        "#Show training data info\n",
        "for region, gdf in training_shapes.items():\n",
        "  print(f\"\\n{region} training data sample:\")\n",
        "  print(gdf.head())"
      ],
      "metadata": {
        "id": "Z7yguTkCbzNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_features=process_all_satellite_data(s1_data, s2_data)\n",
        "\n",
        "print(f\"Processed features shape: {processed_features.shape}\")\n",
        "print(f\"Feature Columns: {processed_features.columns.tolist()}\")\n",
        "print(f\"\\nFirst few row:\")\n",
        "print(processed_features.head())"
      ],
      "metadata": {
        "id": "mO3reqSYc05Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "313f8bbc"
      },
      "source": [
        "from shapely.geometry import Point\n",
        "import geopandas as gpd\n",
        "\n",
        "unique_points = s1_data[['ID', 'translated_lat', 'translated_lon']].drop_duplicates()\n",
        "geometry = [Point(lon, lat) for lon, lat in zip(unique_points['translated_lon'], unique_points['translated_lat'])]\n",
        "points_gdf = gpd.GeoDataFrame(unique_points, geometry=geometry, crs='EPSG:4326') # Explicitly set CRS\n",
        "print(\"\\nCRS of points_gdf:\")\n",
        "print(points_gdf.crs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e741229e"
      },
      "source": [
        "# Perform spatial join with an even larger buffer distance\n",
        "spatial_joined = spatial_join_training_data(s1_data, training_shapes, buffer_distance=0.01) # Increased buffer further\n",
        "\n",
        "if spatial_joined is not None:\n",
        "  print(f\"Spatial join successful: {len(spatial_joined)} points matched\")\n",
        "  print(f\"Columns: {spatial_joined.columns.tolist()}\")\n",
        "else:\n",
        "  print(\"Spatial join failed - check if there is actual spatial overlap or try a larger buffer.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, class_col=prepare_training_dataset(processed_features, spatial_joined)\n",
        "\n",
        "if training_data is not None:\n",
        "  print(f\"Training dataset ready!\")\n",
        "  print(f\"Shape: {training_data.shape}\")\n",
        "  print(f\"Class column: {class_col}\")\n",
        "\n",
        "  #Show class distributio by region\n",
        "  print(f\"\\nClass Distribution by region:\")\n",
        "  print(training_data.groupby(['region', class_col]).size().unstack(fill_value=0))\n",
        "else:\n",
        "  print(\"Training dataset preparation failed\")"
      ],
      "metadata": {
        "id": "GfFYjTAzx-PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if training_data is not None:\n",
        "  model_dict=train_cropland_model(training_data, class_col)\n",
        "  print(\"Model training completed\")\n",
        "else:\n",
        "  print(\"Cannot train model- no training data avalilable!\")"
      ],
      "metadata": {
        "id": "kf1qzJb1Eren"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bUDR1aX_IH7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4af0c289"
      },
      "source": [
        "# Task\n",
        "Save the trained model and generate a submission file named \"SampleSubmission.csv\" using the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d65c2007"
      },
      "source": [
        "## Load test data\n",
        "\n",
        "### Subtask:\n",
        "Load the test data for prediction. Since a separate test file is not provided, we'll use the test set created during the training process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2398d42"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the IDs corresponding to the test set samples from the original training data using the indices from the y_test numpy array.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "915f7f33"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `model_dict` was not defined due to the preceding cell failing. Re-run the training cell first to define `model_dict`, then extract the test set IDs.\n",
        "\n"
      ]
    }
  ]
}